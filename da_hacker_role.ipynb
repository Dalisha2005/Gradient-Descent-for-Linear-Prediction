{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "C6SW_kLhS9u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import platform\n",
        "import os\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score"
      ],
      "metadata": {
        "id": "D03l-c4-S9LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Functions"
      ],
      "metadata": {
        "id": "SHK7zPEnTOc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticLoss:\n",
        "    \"\"\"Implements the Logistic Loss function: phi(p,y) = ln(1 + exp(-py))\"\"\"\n",
        "    def gradient(self, p, y):\n",
        "        \"\"\"phi'(p,y) = -y / (1 + exp(py))\"\"\"\n",
        "        return -y / (1 + np.exp(p * y))\n",
        "\n",
        "class HingeLoss:\n",
        "    \"\"\"Implements the SVM Hinge Loss: phi(p,y) = max(0, 1 - py)\"\"\"\n",
        "    def gradient(self, p, y):\n",
        "        \"\"\"phi'(p,y) = -y if py < 1, 0 otherwise\"\"\"\n",
        "        return -y if (p * y) < 1 else 0"
      ],
      "metadata": {
        "id": "bLJHiHgETRVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Standard SGD Algorithm"
      ],
      "metadata": {
        "id": "Fg1CTYpZTUjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StandardSGD:\n",
        "    \"\"\"Implements Algorithm 2.1 (Standard SGD)\"\"\"\n",
        "    def __init__(self, loss_fn, eta=0.002, lambda_reg=0.0, n_epochs=10):\n",
        "        self.loss_fn = loss_fn\n",
        "        self.eta = eta\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "    def _add_intercept(self, X):\n",
        "        return np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        X_b = self._add_intercept(X)\n",
        "        n_samples, n_features = X_b.shape\n",
        "        self.w_ = np.zeros(n_features)\n",
        "\n",
        "        for _ in range(self.n_epochs):\n",
        "            indices = np.random.permutation(n_samples) # Random ordering\n",
        "            for i in indices:\n",
        "                X_i, y_i = X_b[i], y[i]\n",
        "                p = self.w_ @ X_i\n",
        "                phi_prime = self.loss_fn.gradient(p, y_i)\n",
        "                grad_vector = (self.lambda_reg * self.w_) + (phi_prime * X_i)\n",
        "                self.w_ = self.w_ - self.eta * grad_vector\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        self.comp_time_ms_ = (end_time - start_time) * 1000\n",
        "        return self.comp_time_ms_\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_b = self._add_intercept(X)\n",
        "        p = X_b @ self.w_\n",
        "        return np.sign(p)\n",
        "\n",
        "    def get_weights_norm(self):\n",
        "        return np.linalg.norm(self.w_)"
      ],
      "metadata": {
        "id": "ZJtBLbPwT24c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Average SGD Algorithm"
      ],
      "metadata": {
        "id": "tVehz_6yTkn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AveragedSGD:\n",
        "    \"\"\"Implements Algorithm 5.1 (Averaged SGD)\"\"\"\n",
        "    def __init__(self, loss_fn, eta=0.002, lambda_reg=0.0, n_epochs=10):\n",
        "        self.loss_fn = loss_fn\n",
        "        self.eta = eta\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "    def _add_intercept(self, X):\n",
        "        return np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        X_b = self._add_intercept(X)\n",
        "        n_samples, n_features = X_b.shape\n",
        "\n",
        "        w_t = np.zeros(n_features)\n",
        "        self.v_ = np.zeros(n_features) # This is the final averaged vector\n",
        "        r_t = 0.0\n",
        "\n",
        "        for _ in range(self.n_epochs):\n",
        "            indices = np.random.permutation(n_samples) # Random ordering\n",
        "            for i in indices:\n",
        "                X_i, y_i = X_b[i], y[i]\n",
        "                p = w_t @ X_i\n",
        "                phi_prime = self.loss_fn.gradient(p, y_i)\n",
        "\n",
        "                # Update the average\n",
        "                r_t_prev = r_t\n",
        "                r_t = r_t_prev + self.eta\n",
        "                if r_t > 0:\n",
        "                    self.v_ = (r_t_prev / r_t) * self.v_ + (self.eta / r_t) * w_t\n",
        "\n",
        "                # Update w_t for the *next* iteration\n",
        "                grad_vector = (self.lambda_reg * w_t) + (phi_prime * X_i)\n",
        "                w_t = w_t - self.eta * grad_vector\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        self.comp_time_ms_ = (end_time - start_time) * 1000\n",
        "        return self.comp_time_ms_\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_b = self._add_intercept(X)\n",
        "        p = X_b @ self.v_\n",
        "        return np.sign(p)\n",
        "\n",
        "    def get_weights_norm(self):\n",
        "        return np.linalg.norm(self.v_)"
      ],
      "metadata": {
        "id": "SpKwm1jDTjqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Experiments helper function"
      ],
      "metadata": {
        "id": "fUaHr3oIUNv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model, title, X_train, y_train, X_test, y_test):\n",
        "    print(f\"--- {title} ---\")\n",
        "\n",
        "    # Fit model and get time\n",
        "    comp_time_ms = model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    # Use zero_division=0 to avoid warnings if no positive predictions\n",
        "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "    weight_norm = model.get_weights_norm()\n",
        "\n",
        "    # Print in the requested format\n",
        "    print(f\"  > Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"  > Test Precision: {precision * 100:.2f}%\")\n",
        "    print(f\"  > Time Taken: {comp_time_ms:.2f} ms\")\n",
        "    print(f\"  > Final |w| (L2-Norm): {weight_norm:.4f}\")\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KTHfa60LULtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "ykuIZwb0UQh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, random_state=42)\n",
        "y = (y * 2) - 1  # Map {0, 1} -> {-1, 1}\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                                                    random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "iTS-XgN_USR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Comparisons"
      ],
      "metadata": {
        "id": "MiazzDr9UWMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENT 1: Baseline\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"EXPERIMENT 1: Baseline (Paper's Recommendation)\")\n",
        "print(\"Small eta, No Regularization, Early Stopping (10 epochs)\")\n",
        "print(\"=\"*50)\n",
        "params_baseline = {\"eta\": 0.002, \"lambda_reg\": 0.0, \"n_epochs\": 10}\n",
        "run_experiment(StandardSGD(HingeLoss(), **params_baseline),\n",
        "               \"Standard SGD (SVM Loss)\",\n",
        "               X_train, y_train, X_test, y_test)\n",
        "run_experiment(AveragedSGD(HingeLoss(), **params_baseline),\n",
        "               \"Averaged SGD (SVM Loss)\",\n",
        "               X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyHJdieSUZF_",
        "outputId": "021a7da2-6fdd-495f-bd16-54e7e75ce62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "EXPERIMENT 1: Baseline (Paper's Recommendation)\n",
            "Small eta, No Regularization, Early Stopping (10 epochs)\n",
            "==================================================\n",
            "--- Standard SGD (SVM Loss) ---\n",
            "  > Test Accuracy: 83.00%\n",
            "  > Test Precision: 79.08%\n",
            "  > Time Taken: 90.55 ms\n",
            "  > Final |w| (L2-Norm): 1.7009\n",
            "\n",
            "\n",
            "--- Averaged SGD (SVM Loss) ---\n",
            "  > Test Accuracy: 82.33%\n",
            "  > Test Precision: 78.81%\n",
            "  > Time Taken: 120.61 ms\n",
            "  > Final |w| (L2-Norm): 1.2968\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENT 2: Effect of Large Learning Rate (η)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"EXPERIMENT 2: Effect of Large Learning Rate (eta)\")\n",
        "print(\"Large eta (0.5), No Regularization, 10 epochs\")\n",
        "print(\"=\"*50)\n",
        "params_large_eta = {\"eta\": 0.5, \"lambda_reg\": 0.0, \"n_epochs\": 10}\n",
        "run_experiment(StandardSGD(HingeLoss(), **params_large_eta),\n",
        "               \"Standard SGD (Large eta)\",\n",
        "               X_train, y_train, X_test, y_test)\n",
        "run_experiment(AveragedSGD(HingeLoss(), **params_large_eta),\n",
        "               \"Averaged SGD (Large eta)\",\n",
        "               X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mLaBRwxUf0X",
        "outputId": "30908e92-be74-4ccb-fe1c-55f1e1377523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "EXPERIMENT 2: Effect of Large Learning Rate (eta)\n",
            "Large eta (0.5), No Regularization, 10 epochs\n",
            "==================================================\n",
            "--- Standard SGD (Large eta) ---\n",
            "  > Test Accuracy: 79.67%\n",
            "  > Test Precision: 75.82%\n",
            "  > Time Taken: 89.53 ms\n",
            "  > Final |w| (L2-Norm): 11.7736\n",
            "\n",
            "\n",
            "--- Averaged SGD (Large eta) ---\n",
            "  > Test Accuracy: 83.33%\n",
            "  > Test Precision: 78.85%\n",
            "  > Time Taken: 129.59 ms\n",
            "  > Final |w| (L2-Norm): 10.5280\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENT 3: Regularization (λ) vs. Early Stopping\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"EXPERIMENT 3: Regularization (lambda) vs. Early Stopping\")\n",
        "print(\"=\"*50)\n",
        "print(\"(Run A: Early Stopping - Same as baseline)\")\n",
        "params_early_stop = {\"eta\": 0.002, \"lambda_reg\": 0.0, \"n_epochs\": 10}\n",
        "run_experiment(StandardSGD(HingeLoss(), **params_early_stop),\n",
        "               \"Standard SGD (Early Stop)\",\n",
        "               X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"(Run B: Explicit Regularization - More epochs)\")\n",
        "params_explicit_reg = {\"eta\": 0.002, \"lambda_reg\": 0.01, \"n_epochs\": 100}\n",
        "run_experiment(StandardSGD(HingeLoss(), **params_explicit_reg),\n",
        "               \"Standard SGD (Explicit Lambda)\",\n",
        "               X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAti2OhWUqqQ",
        "outputId": "dec0c921-6752-4b03-f859-cb14426a4f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "EXPERIMENT 3: Regularization (lambda) vs. Early Stopping\n",
            "==================================================\n",
            "(Run A: Early Stopping - Same as baseline)\n",
            "--- Standard SGD (Early Stop) ---\n",
            "  > Test Accuracy: 82.00%\n",
            "  > Test Precision: 77.92%\n",
            "  > Time Taken: 81.78 ms\n",
            "  > Final |w| (L2-Norm): 1.6807\n",
            "\n",
            "\n",
            "(Run B: Explicit Regularization - More epochs)\n",
            "--- Standard SGD (Explicit Lambda) ---\n",
            "  > Test Accuracy: 84.00%\n",
            "  > Test Precision: 79.87%\n",
            "  > Time Taken: 831.47 ms\n",
            "  > Final |w| (L2-Norm): 1.8281\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-0YE-1XSRlU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}